import os
import json
import datetime
from pathlib import Path

# é˜¶æ®µ1: ASR (Paraformer-zh)
from funasr import AutoModel

# é˜¶æ®µ2: æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±» (PyTorch)
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification

# é˜¶æ®µ3: å£°å­¦æƒ…æ„Ÿåˆ†æ (emotion2vec)
# from funasr import AutoModel  # å·²åœ¨ä¸Šé¢å¯¼å…¥

class MultiModelEmotionAnalyzer:
    """ä¸‰é˜¶æ®µå¤šæ¨¡å‹æƒ…æ„Ÿåˆ†æç³»ç»Ÿ"""
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.setup_models()
    
    def setup_models(self):
        """åˆå§‹åŒ–ä¸‰ä¸ªæ¨¡å‹"""
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–ä¸‰é˜¶æ®µæƒ…æ„Ÿåˆ†ææ¨¡å‹...")
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºé…ç½®çŠ¶æ€
        print(f"ğŸ”§ é…ç½®çŠ¶æ€: use_local_models = {self.config_manager.config['settings']['use_local_models']}")
        print(f"ğŸ”§ Paraformer æœ¬åœ°è·¯å¾„: {self.config_manager.get_model_path('paraformer')}")
        print(f"ğŸ”§ emotion2vec æœ¬åœ°è·¯å¾„: {self.config_manager.get_model_path('emotion2vec')}")
        
        # é˜¶æ®µ1: Paraformer-zh ASR
        try:
            paraformer_path = self.config_manager.get_model_path("paraformer")
            if self.config_manager.config["settings"]["use_local_models"]:
                # ä½¿ç”¨æœ¬åœ°æ¨¡å‹ - è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                import os
                paraformer_abs_path = os.path.abspath(paraformer_path)
                print(f"ğŸ”„ å°è¯•åŠ è½½æœ¬åœ°æ¨¡å‹: {paraformer_abs_path}")
                self.paraformer_model = AutoModel(model=paraformer_abs_path)
                print("âœ… é˜¶æ®µ1: Paraformer-zh æœ¬åœ°æ¨¡å‹åŠ è½½æˆåŠŸ")
            else:
                # ä½¿ç”¨åœ¨çº¿æ¨¡å‹ - ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹åç§°
                print("ğŸ”„ å°è¯•åŠ è½½åœ¨çº¿æ¨¡å‹")
                self.paraformer_model = AutoModel(model="iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch")
                print("âœ… é˜¶æ®µ1: Paraformer-zh åœ¨çº¿æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ é˜¶æ®µ1: Paraformer-zh ASR æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.paraformer_model = None
        
        # é˜¶æ®µ2: ä¸­æ–‡æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»
        try:
            if self.config_manager.config["settings"]["use_local_models"]:
                # ä½¿ç”¨æœ¬åœ°ä¸­æ–‡æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹
                local_text_model_path = "distilbert-base-uncased-go-emotions-student"
                print(f"ğŸ”„ å°è¯•åŠ è½½æœ¬åœ°ä¸­æ–‡æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹: {local_text_model_path}")
                self.text_tokenizer = AutoTokenizer.from_pretrained(local_text_model_path)
                self.text_model = AutoModelForSequenceClassification.from_pretrained(local_text_model_path)
            else:
                # ä½¿ç”¨åœ¨çº¿ä¸­æ–‡æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹
                model_name = "uer/roberta-base-finetuned-jd-binary-chinese"
                print(f"ğŸ”„ å°è¯•åŠ è½½åœ¨çº¿ä¸­æ–‡æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹: {model_name}")
                self.text_tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.text_model = AutoModelForSequenceClassification.from_pretrained(model_name)
            
            self.text_pipeline = pipeline(
                "text-classification", 
                model=self.text_model, 
                tokenizer=self.text_tokenizer
            )
            print("âœ… é˜¶æ®µ2: ä¸­æ–‡æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ é˜¶æ®µ2: ä¸­æ–‡æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.text_pipeline = None
        
        # é˜¶æ®µ3: emotion2vec å£°å­¦æƒ…æ„Ÿåˆ†æ
        try:
            emotion2vec_path = self.config_manager.get_model_path("emotion2vec")
            if self.config_manager.config["settings"]["use_local_models"]:
                # ä½¿ç”¨æœ¬åœ°æ¨¡å‹
                self.emotion2vec_model = AutoModel(model=emotion2vec_path)
                print("âœ… é˜¶æ®µ3: emotion2vec æœ¬åœ°æ¨¡å‹åŠ è½½æˆåŠŸ")
            else:
                # ä½¿ç”¨åœ¨çº¿æ¨¡å‹
                self.emotion2vec_model = AutoModel(model="iic/emotion2vec_plus_large")
                print("âœ… é˜¶æ®µ3: emotion2vec åœ¨çº¿æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ é˜¶æ®µ3: emotion2vec å£°å­¦æƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.emotion2vec_model = None
    
    def stage1_asr_transcription(self, audio_path):
        """é˜¶æ®µ1: Paraformer-zh ASR è¯­éŸ³è½¬æ–‡å­—"""
        if not self.paraformer_model:
            return None, "Paraformer-zh æ¨¡å‹æœªåŠ è½½"
        
        try:
            # ä½¿ç”¨ Paraformer-zh è¿›è¡Œè¯­éŸ³è¯†åˆ«
            print(f"ğŸ”„ å¼€å§‹è½¬å½•éŸ³é¢‘: {audio_path}")
            result = self.paraformer_model.generate(
                audio_path,
                output_dir="./temp_outputs",
                batch_size=1
            )
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºåŸå§‹è¾“å‡º
            print(f"ğŸ” ASR åŸå§‹è¾“å‡º: {result}")
            print(f"ğŸ” è¾“å‡ºç±»å‹: {type(result)}")
            if result:
                print(f"ğŸ” è¾“å‡ºé•¿åº¦: {len(result)}")
                if len(result) > 0:
                    print(f"ğŸ” ç¬¬ä¸€ä¸ªå…ƒç´ : {result[0]}")
                    print(f"ğŸ” ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹: {type(result[0])}")
            
            # æå–è½¬å½•æ–‡æœ¬
            if result and len(result) > 0:
                transcription = result[0].get('text', '').strip()
                if transcription:
                    return transcription, None
                else:
                    return None, "ASR è½¬å½•ç»“æœä¸ºç©º"
            else:
                return None, "ASR è½¬å½•ç»“æœä¸ºç©º"
        except Exception as e:
            return None, f"ASR è½¬å½•å¤±è´¥: {e}"
    
    def stage2_text_emotion(self, text):
        """é˜¶æ®µ2: ä¸­æ–‡æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»"""
        if not self.text_pipeline:
            return None, "æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹æœªåŠ è½½"
        
        try:
            # ä¸­æ–‡æƒ…æ„Ÿåˆ†ç±»
            result = self.text_pipeline(text)
            if result and len(result) > 0:
                # æå–æƒ…æ„Ÿæ ‡ç­¾å’Œç½®ä¿¡åº¦
                emotions = []
                for item in result:
                    emotions.append({
                        "label": item["label"],
                        "confidence": item["score"]
                    })
                return emotions, None
            else:
                return None, "æ–‡æœ¬æƒ…æ„Ÿåˆ†æç»“æœä¸ºç©º"
        except Exception as e:
            return None, f"æ–‡æœ¬æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}"
    
    def stage3_audio_emotion(self, audio_path):
        """é˜¶æ®µ3: emotion2vec å£°å­¦æƒ…æ„Ÿåˆ†æ"""
        if not self.emotion2vec_model:
            return None, "emotion2vec æ¨¡å‹æœªåŠ è½½"
        
        try:
            # å£°å­¦æƒ…æ„Ÿåˆ†æ
            result = self.emotion2vec_model.generate(
                audio_path,
                output_dir="./temp_outputs",
                granularity="utterance",
                extract_embedding=False
            )
            
            # è§£æ emotion2vec è¾“å‡º - æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼
            emotions = []
            
            if isinstance(result, list) and len(result) > 0:
                # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼
                for item in result:
                    if isinstance(item, dict):
                        if "emotion" in item:
                            # æ–°æ ¼å¼
                            for emotion_item in item["emotion"]:
                                emotions.append({
                                    "label": emotion_item.get("label", "unknown"),
                                    "confidence": emotion_item.get("score", 0.0)
                                })
                        elif "scores" in item:
                            # æ—§æ ¼å¼ - ä½¿ç”¨é¢„å®šä¹‰çš„æƒ…æ„Ÿæ ‡ç­¾
                            emotion_labels = ['ç”Ÿæ°”/angry', 'åŒæ¶/disgusted', 'ææƒ§/fearful', 'å¼€å¿ƒ/happy', 
                                           'ä¸­ç«‹/neutral', 'å…¶ä»–/other', 'éš¾è¿‡/sad', 'åƒæƒŠ/surprised', '<unk>']
                            scores = item["scores"]
                            for i, score in enumerate(scores):
                                if i < len(emotion_labels):
                                    emotions.append({
                                        "label": emotion_labels[i],
                                        "confidence": score
                                    })
            elif isinstance(result, dict):
                # å¦‚æœæ˜¯å­—å…¸æ ¼å¼
                if "emotion" in result:
                    for emotion_item in result["emotion"]:
                        emotions.append({
                            "label": emotion_item.get("label", "unknown"),
                            "confidence": emotion_item.get("score", 0.0)
                        })
                elif "scores" in result:
                    emotion_labels = ['ç”Ÿæ°”/angry', 'åŒæ¶/disgusted', 'ææƒ§/fearful', 'å¼€å¿ƒ/happy', 
                                   'ä¸­ç«‹/neutral', 'å…¶ä»–/other', 'éš¾è¿‡/sad', 'åƒæƒŠ/surprised', '<unk>']
                    scores = result["scores"]
                    for i, score in enumerate(scores):
                        if i < len(emotion_labels):
                            emotions.append({
                                "label": emotion_labels[i],
                                "confidence": score
                            })
            
            if emotions:
                # æŒ‰ç½®ä¿¡åº¦æ’åº
                emotions.sort(key=lambda x: x["confidence"], reverse=True)
                return emotions, None
            else:
                return None, "æ— æ³•è§£æ emotion2vec è¾“å‡ºæ ¼å¼"
                
        except Exception as e:
            return None, f"å£°å­¦æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}"
    
    def run_three_stage_analysis(self, audio_path):
        """è¿è¡Œä¸‰é˜¶æ®µæƒ…æ„Ÿåˆ†æ"""
        print(f"\nğŸµ å¼€å§‹åˆ†æéŸ³é¢‘æ–‡ä»¶: {audio_path}")
        print("=" * 60)
        
        results = {
            "audio_file": audio_path,
            "timestamp": datetime.datetime.now().isoformat(),
            "stages": {}
        }
        
        # é˜¶æ®µ1: ASR è½¬å½•
        print("ğŸ“ é˜¶æ®µ1: è¯­éŸ³è½¬æ–‡å­— (Paraformer-zh)...")
        transcription, error = self.stage1_asr_transcription(audio_path)
        if error:
            print(f"âŒ è½¬å½•å¤±è´¥: {error}")
            results["stages"]["asr"] = {"status": "failed", "error": error}
        else:
            print(f"âœ… è½¬å½•æˆåŠŸ: {transcription}")
            results["stages"]["asr"] = {
                "status": "success", 
                "transcription": transcription
            }
        
        # é˜¶æ®µ2: æ–‡æœ¬æƒ…æ„Ÿåˆ†æ
        if transcription:
            print("ğŸ’­ é˜¶æ®µ2: æ–‡æœ¬æƒ…æ„Ÿåˆ†æ (ä¸­æ–‡æƒ…æ„Ÿåˆ†ç±»)...")
            text_emotions, error = self.stage2_text_emotion(transcription)
            if error:
                print(f"âŒ æ–‡æœ¬æƒ…æ„Ÿåˆ†æå¤±è´¥: {error}")
                results["stages"]["text_emotion"] = {"status": "failed", "error": error}
            else:
                print(f"âœ… æ–‡æœ¬æƒ…æ„Ÿåˆ†ææˆåŠŸ: {text_emotions}")
                results["stages"]["text_emotion"] = {
                    "status": "success", 
                    "emotions": text_emotions
                }
        
        # é˜¶æ®µ3: å£°å­¦æƒ…æ„Ÿåˆ†æ
        print("ğŸµ é˜¶æ®µ3: å£°å­¦æƒ…æ„Ÿåˆ†æ (emotion2vec)...")
        audio_emotions, error = self.stage3_audio_emotion(audio_path)
        if error:
            print(f"âŒ å£°å­¦æƒ…æ„Ÿåˆ†æå¤±è´¥: {error}")
            results["stages"]["audio_emotion"] = {"status": "failed", "error": error}
        else:
            print(f"âœ… å£°å­¦æƒ…æ„Ÿåˆ†ææˆåŠŸ: {audio_emotions}")
            results["stages"]["audio_emotion"] = {
                "status": "success", 
                "emotions": audio_emotions
            }
        
        # ç»¼åˆåˆ†æç»“æœ
        results["summary"] = self.generate_summary(results)
        
        return results
    
    def generate_summary(self, results):
        """ç”Ÿæˆç»¼åˆåˆ†ææ‘˜è¦"""
        summary = {
            "overall_status": "success",
            "key_findings": [],
            "recommendations": []
        }
        
        # æ£€æŸ¥å„é˜¶æ®µçŠ¶æ€
        failed_stages = []
        for stage_name, stage_result in results["stages"].items():
            if stage_result.get("status") == "failed":
                failed_stages.append(stage_name)
        
        if failed_stages:
            summary["overall_status"] = "partial_success"
            summary["key_findings"].append(f"éƒ¨åˆ†é˜¶æ®µå¤±è´¥: {', '.join(failed_stages)}")
        
        # æå–å…³é”®ä¿¡æ¯
        if "asr" in results["stages"] and results["stages"]["asr"]["status"] == "success":
            transcription = results["stages"]["asr"]["transcription"]
            summary["key_findings"].append(f"è¯­éŸ³å†…å®¹: {transcription}")
        
        # æƒ…æ„Ÿåˆ†æå¯¹æ¯”
        text_emotions = results["stages"].get("text_emotion", {}).get("emotions", [])
        audio_emotions = results["stages"].get("audio_emotion", {}).get("emotions", [])
        
        if text_emotions and audio_emotions:
            summary["key_findings"].append("æ–‡æœ¬ä¸å£°å­¦æƒ…æ„Ÿåˆ†æç»“æœå¯¹æ¯”")
            summary["recommendations"].append("å»ºè®®ç»“åˆä¸¤ç§åˆ†æç»“æœè¿›è¡Œç»¼åˆåˆ¤æ–­")
        
        return summary
    
    def save_results(self, results):
        """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""
        output_dir = self.config_manager.get_output_dir()
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"emotion_analysis_{timestamp}.json"
        filepath = os.path.join(output_dir, filename)
        
        # ä¿å­˜ç»“æœ
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return filepath

class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_path="config.json"):
        self.config_path = config_path
        self.config = self.load_config()
        self.setup_environment()
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if os.path.exists(self.config_path):
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            return self.get_default_config()
    
    def get_default_config(self):
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "paths": {
                "input_dir": "input",
                "output_dir": "outputs",
                "temp_dir": "temp_outputs",
                "models_cache": "./models_cache"
            },
            "models": {
                "emotion2vec": {
                    "name": "iic/emotion2vec_plus_large",
                    "type": "funasr",
                    "local_path": "emotion2vec_plus_large"
                },
                "paraformer": {
                    "name": "paraformer-zh",
                    "type": "funasr",
                    "local_path": "paraformer-zh"
                }
            },
            "settings": {
                "use_local_models": True,
                "download_to_project": True
            },
            "audio_files": {
                "default": "asr_example.wav",
                "available": ["asr_example.wav", "example.wav"]
            }
        }
    
    def setup_environment(self):
        """è®¾ç½®ç¯å¢ƒå˜é‡"""
        cache_dir = self.config["paths"]["models_cache"]
        os.environ['HF_HOME'] = cache_dir
        os.environ['MODELSCOPE_CACHE'] = cache_dir
    
    def get_model_path(self, model_name):
        """è·å–æ¨¡å‹è·¯å¾„"""
        if model_name in self.config["models"]:
            return self.config["models"][model_name]["local_path"]
        return None
    
    def get_output_dir(self):
        """è·å–è¾“å‡ºç›®å½•"""
        return self.config["paths"]["output_dir"]
    
    def get_audio_path(self, filename):
        """è·å–éŸ³é¢‘æ–‡ä»¶è·¯å¾„"""
        input_dir = self.config["paths"]["input_dir"]
        return os.path.join(input_dir, filename)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ­ ä¸‰é˜¶æ®µå¤šæ¨¡å‹æƒ…æ„Ÿåˆ†æç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
    config_manager = ConfigManager()
    
    # åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨
    analyzer = MultiModelEmotionAnalyzer(config_manager)
    
    # è·å–éŸ³é¢‘æ–‡ä»¶è·¯å¾„ - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤æ–‡ä»¶
    audio_file = config_manager.config["audio_files"]["default"]
    audio_path = config_manager.get_audio_path(audio_file)
    
    print(f"ğŸµ ä½¿ç”¨éŸ³é¢‘æ–‡ä»¶: {audio_file}")
    print(f"ğŸµ å®Œæ•´è·¯å¾„: {audio_path}")
    
    if not os.path.exists(audio_path):
        print(f"âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
        print(f"ğŸ“ å¯ç”¨çš„éŸ³é¢‘æ–‡ä»¶: {config_manager.config['audio_files']['available']}")
        return
    
    # è¿è¡Œä¸‰é˜¶æ®µåˆ†æ
    results = analyzer.run_three_stage_analysis(audio_path)
    
    # ä¿å­˜ç»“æœ
    output_file = analyzer.save_results(results)
    
    # æ‰“å°æ‘˜è¦
    print("\nğŸ“Š åˆ†ææ‘˜è¦:")
    print("-" * 30)
    summary = results["summary"]
    print(f"æ•´ä½“çŠ¶æ€: {summary['overall_status']}")
    for finding in summary["key_findings"]:
        print(f"â€¢ {finding}")
    for recommendation in summary["recommendations"]:
        print(f"ğŸ’¡ {recommendation}")

if __name__ == "__main__":
    main()