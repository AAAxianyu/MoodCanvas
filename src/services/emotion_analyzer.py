"""
ä¸‰é˜¶æ®µå¤šæ¨¡å‹æƒ…æ„Ÿåˆ†ææœåŠ¡
"""
import os
import json
import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime, timezone
import time
import logging

from src.models.emotion.text_emotion import TextEmotionModel
from src.models.emotion.audio_emotion import AudioEmotionModel
from src.models.asr.paraformer import ParaformerModel
from src.models.image.text2image import ImageGenerator
from src.models.image.image2image import ImageEditor
from src.core.config_manager import ConfigManager
from src.core.exceptions import EmotionAnalysisError, AudioProcessingError, GenerationError
from src.services.text_generator import TextGenerator

logger = logging.getLogger(__name__)

class MultiModelEmotionAnalyzer:
    """ä¸‰é˜¶æ®µå¤šæ¨¡å‹æƒ…æ„Ÿåˆ†æç³»ç»Ÿ"""
    
    EMOTION2VEC_LABELS = {0: "angry",1: "disgusted",2: "fearful",3: "happy",4: "neutral",5: "other",6: "sad",7: "surprised",8: "unknown"}
    TEXT_EMOTION_LABELS = {
        0: "admiration", 1: "amusement", 2: "anger", 3: "annoyance", 4: "approval",
        5: "caring", 6: "confusion", 7: "curiosity", 8: "desire", 9: "disappointment",
        10: "disapproval", 11: "disgust", 12: "embarrassment", 13: "excitement", 14: "fear",
        15: "gratitude", 16: "grief", 17: "joy", 18: "love", 19: "nervousness",
        20: "optimism", 21: "pride", 22: "realization", 23: "relief", 24: "remorse",
        25: "sadness", 26: "surprise", 27: "neutral"
    }

    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.asr_model = None
        self.text_emotion_model = None
        self.audio_emotion_model = None
        self.image_generator = ImageGenerator(config_manager)
        self.image_editor = None
        self.text_generator = None
        self.setup_models()
    
    def setup_models(self):
        """åˆå§‹åŒ–ä¸‰ä¸ªæ¨¡å‹"""
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–ä¸‰é˜¶æ®µæƒ…æ„Ÿåˆ†ææ¨¡å‹...")
        use_local = self.config_manager.config.get("settings", {}).get("use_local_models", True)
        
        # é˜¶æ®µ1: Paraformer-zh ASR
        try:
            paraformer_config = self.config_manager.config.get("models", {}).get("paraformer", {})
            paraformer_config["use_local_models"] = use_local
            
            # ä¼˜å…ˆä½¿ç”¨model_idï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨name
            model_id = paraformer_config.get("model_id", paraformer_config.get("name"))
            if model_id:
                paraformer_config["model_id"] = model_id
            
            self.asr_model = ParaformerModel(paraformer_config)
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰load_modelæ–¹æ³•ï¼Œå¦‚æœæ²¡æœ‰åˆ™è·³è¿‡
            if hasattr(self.asr_model, 'load_model'):
                if self.asr_model.load_model():
                    print("âœ… é˜¶æ®µ1: Paraformer-zh ASR æ¨¡å‹åŠ è½½æˆåŠŸ")
                else:
                    print("âŒ é˜¶æ®µ1: Paraformer-zh ASR æ¨¡å‹åŠ è½½å¤±è´¥")
            else:
                print("âš ï¸  é˜¶æ®µ1: Paraformer-zh ASR æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ˆæ— load_modelæ–¹æ³•ï¼‰")
        except Exception as e:
            print(f"âŒ é˜¶æ®µ1: Paraformer-zh ASR æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.asr_model = None
        
        # é˜¶æ®µ2: æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»
        try:
            text_emotion_config = self.config_manager.config.get("models", {}).get("text_emotion", {})
            text_emotion_config["use_local_models"] = use_local
            
            # ä¼˜å…ˆä½¿ç”¨model_idï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨name
            model_id = text_emotion_config.get("model_id", text_emotion_config.get("name"))
            if model_id:
                text_emotion_config["model_id"] = model_id
            
            self.text_emotion_model = TextEmotionModel(text_emotion_config)
            if hasattr(self.text_emotion_model, 'load_model'):
                if self.text_emotion_model.load_model():
                    print("âœ… é˜¶æ®µ2: æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹åŠ è½½æˆåŠŸ")
                else:
                    print("âŒ é˜¶æ®µ2: æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹åŠ è½½å¤±è´¥")
            else:
                print("âš ï¸  é˜¶æ®µ2: æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ˆæ— load_modelæ–¹æ³•ï¼‰")
        except Exception as e:
            print(f"âŒ é˜¶æ®µ2: æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.text_emotion_model = None
        
        # é˜¶æ®µ3: emotion2vec å£°å­¦æƒ…æ„Ÿåˆ†æ
        try:
            audio_emotion_config = self.config_manager.config.get("models", {}).get("emotion2vec", {})
            audio_emotion_config["use_local_models"] = use_local
            
            # ä¼˜å…ˆä½¿ç”¨model_idï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨name
            model_id = audio_emotion_config.get("model_id", audio_emotion_config.get("name"))
            if model_id:
                audio_emotion_config["model_id"] = model_id
            
            self.audio_emotion_model = AudioEmotionModel(audio_emotion_config)
            if hasattr(self.audio_emotion_model, 'load_model'):
                if self.audio_emotion_model.load_model():
                    print("âœ… é˜¶æ®µ3: emotion2vec å£°å­¦æƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½æˆåŠŸ")
                else:
                    print("âŒ é˜¶æ®µ3: emotion2vec å£°å­¦æƒ…æ„Ÿåˆ†ææ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
            else:
                print("âš ï¸  é˜¶æ®µ3: emotion2vec å£°å­¦æƒ…æ„Ÿåˆ†ææ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ˆæ— load_modelæ–¹æ³•ï¼‰")
        except Exception as e:
            print(f"âŒ é˜¶æ®µ3: emotion2vec å£°å­¦æƒ…æ„Ÿåˆ†ææ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.audio_emotion_model = None

    def _load_models(self):
        """åŠ è½½æ‰€æœ‰æ¨¡å‹ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        try:
            if self.text_emotion_model and hasattr(self.text_emotion_model, 'load_model'):
                if not self.text_emotion_model.load_model():
                    logger.warning("æ–‡æœ¬æƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½å¤±è´¥")
            if self.audio_emotion_model and hasattr(self.audio_emotion_model, 'load_model'):
                if not self.audio_emotion_model.load_model():
                    logger.warning("éŸ³é¢‘æƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½å¤±è´¥")
            if self.asr_model and hasattr(self.asr_model, 'load_model'):
                if not self.asr_model.load_model():
                    logger.warning("ASRæ¨¡å‹åŠ è½½å¤±è´¥")
            logger.info("æ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
    
    async def process_text_service(self, text: str, language: str = "zh", style_preference: Optional[str] = None) -> Dict[str, Any]:
        start_time = time.time()
        try:
            logger.info(f"å¼€å§‹å¤„ç†æ–‡å­—æƒ…æ„Ÿåˆ†æï¼Œè¾“å…¥é•¿åº¦: {len(text)}")
            if not self.text_emotion_model:
                raise EmotionAnalysisError("æ–‡æœ¬æƒ…æ„Ÿåˆ†ææ¨¡å‹æœªåˆå§‹åŒ–")
            
            emotion_result = self.text_emotion_model.analyze(text)
            if not emotion_result:
                raise EmotionAnalysisError("æ–‡æœ¬æƒ…æ„Ÿåˆ†æå¤±è´¥ï¼Œæœªè·å¾—æœ‰æ•ˆç»“æœ")
            
            emotion_tags = self._extract_text_emotion_tags(emotion_result)
            confidence = self._extract_confidence(emotion_result)
            generated_text = await self._generate_text_with_llm(text, emotion_tags, style_preference, language)
            
            image_path = None
            if self.image_generator:
                try:
                    image_prompt = self._build_image_prompt(text, emotion_tags, generated_text)
                    image_result = self.image_generator.generate(prompt=image_prompt, save_local=True)
                    image_path = image_result['local_paths'][0] if image_result and image_result.get('local_paths') else None
                except Exception as e:
                    logger.warning(f"å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {e}")
            
            processing_time = time.time() - start_time
            # æ„å»ºå®Œæ•´çš„å›¾ç‰‡URL
            image_url = None
            if image_path:
                static_prefix = "/static/generated"
                try:
                    # å¤„ç†ç»å¯¹è·¯å¾„å’Œç›¸å¯¹è·¯å¾„
                    abs_image_path = Path(image_path).resolve()
                    abs_gen_dir = Path("data/generated_images").resolve()
                    rel_path = abs_image_path.relative_to(abs_gen_dir)
                    image_url = f"{static_prefix}/{str(rel_path).replace('\\', '/')}"
                except ValueError as e:
                    logger.warning(f"å›¾ç‰‡è·¯å¾„è½¬æ¢å¤±è´¥: {e}")
                    image_url = None

            result = {
                'text': text,
                'emotion_tags': emotion_tags,
                'emotion_confidence': confidence,
                'generated_content': {
                    'text': generated_text,
                    'image_path': image_path,
                    'image_url': image_url,
                    'style': style_preference or 'default',
                    'metadata': {
                        'language': language,
                        'style_preference': style_preference,
                        'generation_timestamp': datetime.now(timezone.utc).isoformat()
                    }
                },
                'processing_time': round(processing_time, 3),
                'status': 'success'
            }
            return result
        except Exception as e:
            logger.error(f"æ–‡å­—å¤„ç†æœåŠ¡å¤±è´¥: {str(e)}", exc_info=True)
            raise EmotionAnalysisError(f"æ–‡å­—å¤„ç†å¤±è´¥: {str(e)}")
    
    async def process_audio_service(self, audio_data: bytes, language: str = "zh", enable_dual_analysis: bool = True, fusion_strategy: str = "weighted") -> Dict[str, Any]:
        start_time = time.time()
        temp_audio_path = None
        try:
            temp_audio_path = self._save_temp_audio(audio_data)
            
            if not self.asr_model:
                raise AudioProcessingError("ASRæ¨¡å‹æœªåˆå§‹åŒ–")
            
            transcribed_text = self.asr_model.transcribe(temp_audio_path)
            if not transcribed_text:
                raise AudioProcessingError("è¯­éŸ³è¯†åˆ«å¤±è´¥")
            
            audio_emotion_tags = ['neutral']  # é»˜è®¤å€¼
            if self.audio_emotion_model:
                try:
                    audio_emotion_result = self.audio_emotion_model.analyze(temp_audio_path)
                    audio_emotion_tags = self._extract_audio_emotion_tags(audio_emotion_result)
                except Exception as e:
                    logger.warning(f"éŸ³é¢‘æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            
            text_emotion_tags = ['neutral']  # é»˜è®¤å€¼
            if self.text_emotion_model:
                try:
                    text_emotion_result = self.text_emotion_model.analyze(transcribed_text)
                    text_emotion_tags = self._extract_text_emotion_tags(text_emotion_result)
                except Exception as e:
                    logger.warning(f"æ–‡æœ¬æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            
            merged_emotion = self._fuse_emotions(audio_emotion_tags, text_emotion_tags, fusion_strategy)
            generated_text = await self._generate_text_with_llm(transcribed_text, merged_emotion, None, language)
            
            image_path = None
            if self.image_generator:
                try:
                    image_prompt = self._build_image_prompt(transcribed_text, merged_emotion, generated_text)
                    image_result = self.image_generator.generate(prompt=image_prompt, save_local=True)
                    image_path = image_result['local_paths'][0] if image_result and image_result.get('local_paths') else None
                except Exception as e:
                    logger.warning(f"å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {e}")
            
            processing_time = time.time() - start_time
            # æ„å»ºå®Œæ•´çš„å›¾ç‰‡URL
            image_url = None
            if image_path:
                static_prefix = "/static/generated"
                try:
                    # å¤„ç†ç»å¯¹è·¯å¾„å’Œç›¸å¯¹è·¯å¾„
                    abs_image_path = Path(image_path).resolve()
                    abs_gen_dir = Path("data/generated_images").resolve()
                    rel_path = abs_image_path.relative_to(abs_gen_dir)
                    image_url = f"{static_prefix}/{str(rel_path).replace('\\', '/')}"
                except ValueError as e:
                    logger.warning(f"å›¾ç‰‡è·¯å¾„è½¬æ¢å¤±è´¥: {e}")
                    image_url = None

            result = {
                'transcribed_text': transcribed_text,
                'emotion_analysis': {
                    'audio_emotion': audio_emotion_tags,
                    'text_emotion': text_emotion_tags,
                    'merged_emotion': merged_emotion,
                    'fusion_rules': fusion_strategy
                },
                'generated_content': {
                    'text': generated_text,
                    'image_path': image_path,
                    'image_url': image_url,
                    'style': 'default',
                    'metadata': {
                        'language': language,
                        'dual_analysis': enable_dual_analysis,
                        'fusion_strategy': fusion_strategy,
                        'generation_timestamp': datetime.now(timezone.utc).isoformat()
                    }
                },
                'processing_time': round(processing_time, 3),
                'status': 'success'
            }
            return result
        except Exception as e:
            logger.error(f"éŸ³é¢‘å¤„ç†æœåŠ¡å¤±è´¥: {str(e)}", exc_info=True)
            raise AudioProcessingError(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {str(e)}")
        finally:
            if temp_audio_path and os.path.exists(temp_audio_path):
                self._cleanup_temp_file(temp_audio_path)

    def _extract_audio_emotion_tags(self, emotion_result: List[Dict[str, Any]]) -> List[str]:
        tags = []
        try:
            for item in emotion_result:
                if isinstance(item, dict) and 'raw_result' in item:
                    raw = item['raw_result']
                    if isinstance(raw, dict):
                        if 'emotion' in raw:
                            emotion_id = raw['emotion']
                            if isinstance(emotion_id, int) and emotion_id in self.EMOTION2VEC_LABELS:
                                tags.append(self.EMOTION2VEC_LABELS[emotion_id])
                        elif 'emotions' in raw:
                            emotions = raw['emotions']
                            if isinstance(emotions, list):
                                for emotion in emotions[:3]:
                                    if isinstance(emotion, dict) and 'emotion' in emotion:
                                        emotion_id = emotion['emotion']
                                        if isinstance(emotion_id, int) and emotion_id in self.EMOTION2VEC_LABELS:
                                            tags.append(self.EMOTION2VEC_LABELS[emotion_id])
        except Exception as e:
            logger.warning(f"æå–éŸ³é¢‘æƒ…æ„Ÿæ ‡ç­¾å¤±è´¥: {str(e)}")
        return tags[:3] if tags else ['neutral']

    def _extract_text_emotion_tags(self, emotion_result: List[Dict[str, Any]]) -> List[str]:
        tags = []
        try:
            for item in emotion_result:
                if isinstance(item, dict):
                    if 'label' in item:
                        label = item['label']
                        if label in self.TEXT_EMOTION_LABELS.values():
                            tags.append(label)
                    elif 'emotion' in item:
                        emotion = item['emotion']
                        if emotion in self.TEXT_EMOTION_LABELS.values():
                            tags.append(emotion)
        except Exception as e:
            logger.warning(f"æå–æ–‡æœ¬æƒ…æ„Ÿæ ‡ç­¾å¤±è´¥: {str(e)}")
        return tags[:3] if tags else ['neutral']

    def _extract_confidence(self, emotion_result: List[Dict[str, Any]]) -> float:
        try:
            for item in emotion_result:
                if isinstance(item, dict) and 'score' in item:
                    return float(item['score'])
        except Exception:
            pass
        return 0.8

    def _build_image_prompt(self, text: str, emotion_tags: List[str], generated_text: str) -> str:
        emotion_desc = ", ".join(emotion_tags)
        return f"åŸºäºæ–‡å­—'{text}'å’Œæƒ…æ„Ÿ'{emotion_desc}'ï¼Œç”Ÿæˆä¸æ–‡æ¡ˆ'{generated_text}'ç›¸åŒ¹é…çš„å›¾ç‰‡"

    def _fuse_emotions(self, audio_emotions: List[str], text_emotions: List[str], strategy: str) -> List[str]:
        if strategy == "max":
            return audio_emotions if len(audio_emotions) > len(text_emotions) else text_emotions
        elif strategy == "average":
            intersection = set(audio_emotions) & set(text_emotions)
            if intersection:
                return list(intersection)
            else:
                return list(set(audio_emotions + text_emotions))
        else:  # weighted
            all_emotions = audio_emotions + text_emotions
            emotion_count = {}
            for emotion in all_emotions:
                if emotion in audio_emotions and emotion in text_emotions:
                    emotion_count[emotion] = emotion_count.get(emotion, 0) + 2
                else:
                    emotion_count[emotion] = emotion_count.get(emotion, 0) + 1
            sorted_emotions = sorted(emotion_count.items(), key=lambda x: x[1], reverse=True)
            return [emotion for emotion, _ in sorted_emotions[:3]]
    
    async def _generate_text_with_llm(self, text: str, emotion_tags: List[str], style: Optional[str], language: str) -> str:
        emotion_desc = ", ".join(emotion_tags)
        style_desc = f"é£æ ¼ï¼š{style}" if style else ""
        return f"åŸºäºæ‚¨çš„æƒ…æ„Ÿ'{emotion_desc}'ï¼Œæˆ‘ä¸º'{text}'åˆ›ä½œäº†è¿™æ®µæ–‡æ¡ˆï¼šåœ¨{emotion_desc}çš„æ—‹å¾‹ä¸­ï¼Œ{text}ä»¿ä½›æœ‰äº†æ–°çš„ç”Ÿå‘½..."

    def _save_temp_audio(self, audio_data: bytes) -> str:
        import tempfile
        temp_dir = "data/temp"
        os.makedirs(temp_dir, exist_ok=True)
        temp_file = tempfile.NamedTemporaryFile(dir=temp_dir, suffix=".wav", delete=False)
        temp_file.write(audio_data)
        temp_file.close()
        return temp_file.name

    def _cleanup_temp_file(self, file_path: str):
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
                logger.info(f"ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†: {file_path}")
        except Exception as e:
            logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")

    async def run_three_stage_analysis(self, audio_path: str) -> Dict[str, Any]:
        """è¿è¡Œä¸‰é˜¶æ®µæƒ…æ„Ÿåˆ†æ"""
        start_time = time.time()
        try:
            # é˜¶æ®µ1: ASRè½¬å½•
            if not self.asr_model:
                raise AudioProcessingError("ASRæ¨¡å‹æœªåˆå§‹åŒ–")
            
            transcribed_text = self.asr_model.transcribe(audio_path)
            if not transcribed_text:
                raise AudioProcessingError("è¯­éŸ³è¯†åˆ«å¤±è´¥")
            
            # é˜¶æ®µ2: æ–‡æœ¬æƒ…æ„Ÿåˆ†æ
            text_emotion_tags = ['neutral']  # é»˜è®¤å€¼
            if self.text_emotion_model:
                try:
                    text_emotion_result = self.text_emotion_model.analyze(transcribed_text)
                    text_emotion_tags = self._extract_text_emotion_tags(text_emotion_result)
                except Exception as e:
                    logger.warning(f"æ–‡æœ¬æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            
            # é˜¶æ®µ3: éŸ³é¢‘æƒ…æ„Ÿåˆ†æ
            audio_emotion_tags = ['neutral']  # é»˜è®¤å€¼
            if self.audio_emotion_model:
                try:
                    audio_emotion_result = self.audio_emotion_model.analyze(audio_path)
                    audio_emotion_tags = self._extract_audio_emotion_tags(audio_emotion_result)
                except Exception as e:
                    logger.warning(f"éŸ³é¢‘æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            
            # èåˆæƒ…æ„Ÿæ ‡ç­¾
            merged_emotion = self._fuse_emotions(audio_emotion_tags, text_emotion_tags, "weighted")
            
            # ç”Ÿæˆæ–‡æ¡ˆå’Œå›¾ç‰‡
            generated_content = await self._generate_content(transcribed_text, merged_emotion)
            
            processing_time = time.time() - start_time
            
            return {
                'transcribed_text': transcribed_text,
                'emotion_analysis': {
                    'audio_emotion': audio_emotion_tags,
                    'text_emotion': text_emotion_tags,
                    'merged_emotion': merged_emotion,
                    'fusion_strategy': 'weighted'
                },
                'generated_content': generated_content,
                'processing_time': round(processing_time, 3),
                'status': 'success'
            }
            
        except Exception as e:
            logger.error(f"ä¸‰é˜¶æ®µæƒ…æ„Ÿåˆ†æå¤±è´¥: {str(e)}", exc_info=True)
            raise EmotionAnalysisError(f"æƒ…æ„Ÿåˆ†æå¤±è´¥: {str(e)}")

    async def run_text_emotion_analysis(self, text: str) -> Dict[str, Any]:
        """è¿è¡Œçº¯æ–‡æœ¬æƒ…æ„Ÿåˆ†æ"""
        start_time = time.time()
        try:
            # æ–‡æœ¬æƒ…æ„Ÿåˆ†æ
            if not self.text_emotion_model:
                raise EmotionAnalysisError("æ–‡æœ¬æƒ…æ„Ÿåˆ†ææ¨¡å‹æœªåˆå§‹åŒ–")
            
            text_emotion_result = self.text_emotion_model.analyze(text)
            text_emotion_tags = self._extract_text_emotion_tags(text_emotion_result)
            
            # ç”Ÿæˆæ–‡æ¡ˆå’Œå›¾ç‰‡
            generated_content = await self._generate_content(text, text_emotion_tags)
            
            processing_time = time.time() - start_time
            
            return {
                'input_text': text,
                'emotion_analysis': {
                    'text_emotion': text_emotion_tags,
                    'confidence': self._extract_confidence(text_emotion_result)
                },
                'generated_content': generated_content,
                'processing_time': round(processing_time, 3),
                'status': 'success'
            }
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬æƒ…æ„Ÿåˆ†æå¤±è´¥: {str(e)}", exc_info=True)
            raise EmotionAnalysisError(f"æ–‡æœ¬æƒ…æ„Ÿåˆ†æå¤±è´¥: {str(e)}")

    async def _generate_content(self, text: str, emotion_tags: List[str]) -> Dict[str, Any]:
        """ç”Ÿæˆæ–‡æ¡ˆå’Œå›¾ç‰‡å†…å®¹"""
        try:
            # ç”Ÿæˆæ–‡æ¡ˆ
            generated_text = ""
            if self.text_generator:
                try:
                    generated_text = await self.text_generator.generate_text(text, emotion_tags)
                except Exception as e:
                    logger.warning(f"æ–‡æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
                    generated_text = f"åŸºäº'{text}'çš„æƒ…æ„Ÿ'{', '.join(emotion_tags)}'ï¼Œæˆ‘ä¸ºæ‚¨åˆ›ä½œäº†è¿™æ®µæ–‡æ¡ˆï¼šåœ¨{', '.join(emotion_tags)}çš„æ—‹å¾‹ä¸­ï¼Œ{text}ä»¿ä½›æœ‰äº†æ–°çš„ç”Ÿå‘½..."
            else:
                generated_text = f"åŸºäº'{text}'çš„æƒ…æ„Ÿ'{', '.join(emotion_tags)}'ï¼Œæˆ‘ä¸ºæ‚¨åˆ›ä½œäº†è¿™æ®µæ–‡æ¡ˆï¼šåœ¨{', '.join(emotion_tags)}çš„æ—‹å¾‹ä¸­ï¼Œ{text}ä»¿ä½›æœ‰äº†æ–°çš„ç”Ÿå‘½..."
            
            # ç”Ÿæˆå›¾ç‰‡
            image_path = None
            if self.image_generator:
                try:
                    image_prompt = self._build_image_prompt(text, emotion_tags, generated_text)
                    image_result = self.image_generator.generate(prompt=image_prompt, save_local=True)
                    image_path = image_result['local_paths'][0] if image_result and image_result.get('local_paths') else None
                except Exception as e:
                    logger.warning(f"å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {e}")
            
            # æ„å»ºå®Œæ•´çš„å›¾ç‰‡URL
            image_url = None
            if image_path:
                static_prefix = "/static/generated"
                try:
                    # å¤„ç†ç»å¯¹è·¯å¾„å’Œç›¸å¯¹è·¯å¾„
                    abs_image_path = Path(image_path).resolve()
                    abs_gen_dir = Path("data/generated_images").resolve()
                    rel_path = abs_image_path.relative_to(abs_gen_dir)
                    image_url = f"{static_prefix}/{str(rel_path).replace('\\', '/')}"
                except ValueError as e:
                    logger.warning(f"å›¾ç‰‡è·¯å¾„è½¬æ¢å¤±è´¥: {e}")
                    image_url = None

            return {
                'text': generated_text,
                'image_path': image_path,
                'image_url': image_url,
                'style': 'default',
                'metadata': {
                    'generation_timestamp': datetime.now(timezone.utc).isoformat(),
                    'emotion_tags': emotion_tags
                }
            }
            
        except Exception as e:
            logger.error(f"å†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}")
            return {
                'text': f"åŸºäº'{text}'çš„æƒ…æ„Ÿåˆ†æç»“æœç”Ÿæˆæ–‡æ¡ˆå¤±è´¥",
                'image_path': None,
                'style': 'default',
                'metadata': {
                    'generation_timestamp': datetime.now(timezone.utc).isoformat(),
                    'emotion_tags': emotion_tags,
                    'error': str(e)
                }
            }
