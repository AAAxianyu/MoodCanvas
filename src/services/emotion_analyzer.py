"""
ä¸‰é˜¶æ®µå¤šæ¨¡åž‹æƒ…æ„Ÿåˆ†æžæœåŠ¡
"""
import os
import json
import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
from PIL import Image
from clip_interrogator import Config as CIConfig, Interrogator
import io

import logging
from datetime import datetime, timezone
import time
import base64
import requests

from src.models.emotion.text_emotion import TextEmotionModel
from src.models.emotion.audio_emotion import AudioEmotionModel
from src.models.image.image2image import ImageEditor
from src.models.image.text2image import ImageGenerator
from src.models.asr.paraformer import ParaformerModel
from src.utils.file_utils import save_upload_file

from src.core.config_manager import ConfigManager
from src.core.exceptions import EmotionAnalysisError, AudioProcessingError, GenerationError, ImageProcessingError, FileValidationError
from src.services.text_generator import TextGenerator
from src.utils.image_utils import validate_image_file

logger = logging.getLogger(__name__)

class MultiModelEmotionAnalyzer:
    """ä¸‰é˜¶æ®µå¤šæ¨¡åž‹æƒ…æ„Ÿåˆ†æžç³»ç»Ÿ"""
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.asr_model = None
        self.text_emotion_model = None
        self.audio_emotion_model = None
        self.setup_models()
    
    def setup_models(self):
        """åˆå§‹åŒ–ä¸‰ä¸ªæ¨¡åž‹"""
        print("ðŸš€ æ­£åœ¨åˆå§‹åŒ–ä¸‰é˜¶æ®µæƒ…æ„Ÿåˆ†æžæ¨¡åž‹...")
        use_local = self.config_manager.config.get("settings", {}).get("use_local_models", True)
        
        # é˜¶æ®µ1: Paraformer-zh ASR
        try:
            paraformer_config = self.config_manager.config.get("models", {}).get("paraformer", {})
            paraformer_config["use_local_models"] = use_local
            self.asr_model = ParaformerModel(paraformer_config)
            if self.asr_model.load_model():
                print("âœ… é˜¶æ®µ1: Paraformer-zh ASR æ¨¡åž‹åŠ è½½æˆåŠŸ")
            else:
                print("âŒ é˜¶æ®µ1: Paraformer-zh ASR æ¨¡åž‹åŠ è½½å¤±è´¥")
        except Exception as e:
            print(f"âŒ é˜¶æ®µ1: Paraformer-zh ASR æ¨¡åž‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.asr_model = None
        
        # é˜¶æ®µ2: æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»
        try:
            text_emotion_config = self.config_manager.config.get("models", {}).get("text_emotion", {})
            text_emotion_config["use_local_models"] = use_local
            self.text_emotion_model = TextEmotionModel(text_emotion_config)
            if self.text_emotion_model.load_model():
                print("âœ… é˜¶æ®µ2: æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»æ¨¡åž‹åŠ è½½æˆåŠŸ")
            else:
                print("âŒ é˜¶æ®µ2: æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»æ¨¡åž‹åŠ è½½å¤±è´¥")
        except Exception as e:
            print(f"âŒ é˜¶æ®µ2: æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»æ¨¡åž‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.text_emotion_model = None
        
        # é˜¶æ®µ3: emotion2vec å£°å­¦æƒ…æ„Ÿåˆ†æž
        try:
            audio_emotion_config = self.config_manager.config.get("models", {}).get("emotion2vec", {})
            audio_emotion_config["use_local_models"] = use_local
            self.audio_emotion_model = AudioEmotionModel(audio_emotion_config)
            if self.audio_emotion_model.load_model():
                print("âœ… é˜¶æ®µ3: emotion2vec å£°å­¦æƒ…æ„Ÿåˆ†æžæ¨¡åž‹åŠ è½½æˆåŠŸ")
            else:
                print("âŒ é˜¶æ®µ3: emotion2vec å£°å­¦æƒ…æ„Ÿåˆ†æžæ¨¡åž‹åˆå§‹åŒ–å¤±è´¥")
        except Exception as e:
            print(f"âŒ é˜¶æ®µ3: emotion2vec å£°å­¦æƒ…æ„Ÿåˆ†æžæ¨¡åž‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.audio_emotion_model = None

class EmotionAnalyzerService:
    EMOTION2VEC_LABELS = {0: "angry",1: "disgusted",2: "fearful",3: "happy",4: "neutral",5: "other",6: "sad",7: "surprised",8: "unknown"}
    TEXT_EMOTION_LABELS = {
        0: "admiration", 1: "amusement", 2: "anger", 3: "annoyance", 4: "approval",
        5: "caring", 6: "confusion", 7: "curiosity", 8: "desire", 9: "disappointment",
        10: "disapproval", 11: "disgust", 12: "embarrassment", 13: "excitement", 14: "fear",
        15: "gratitude", 16: "grief", 17: "joy", 18: "love", 19: "nervousness",
        20: "optimism", 21: "pride", 22: "realization", 23: "relief", 24: "remorse",
        25: "sadness", 26: "surprise", 27: "neutral"
    }

    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.asr_model = None
        self.text_emotion_model = None
        self.audio_emotion_model = None
        self.image_generator = None
        self.image_editor = None
        self.text_generator = None
        self.setup_models()
    
    def setup_models(self):
        """åˆå§‹åŒ–ä¸‰ä¸ªæ¨¡åž‹"""
        print("ðŸš€ æ­£åœ¨åˆå§‹åŒ–ä¸‰é˜¶æ®µæƒ…æ„Ÿåˆ†æžæ¨¡åž‹...")
        use_local = self.config_manager.config.get("settings", {}).get("use_local_models", True)
        
        # é˜¶æ®µ1: Paraformer-zh ASR
        try:
            paraformer_config = self.config_manager.config.get("models", {}).get("paraformer", {})
            paraformer_config["use_local_models"] = use_local
            self.asr_model = ParaformerModel(paraformer_config)
            if self.asr_model.load_model():
                print("âœ… é˜¶æ®µ1: Paraformer-zh ASR æ¨¡åž‹åŠ è½½æˆåŠŸ")
            else:
                print("âŒ é˜¶æ®µ1: Paraformer-zh ASR æ¨¡åž‹åŠ è½½å¤±è´¥")
        except Exception as e:
            print(f"âŒ é˜¶æ®µ1: Paraformer-zh ASR æ¨¡åž‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.asr_model = None
        
        # é˜¶æ®µ2: æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»
        try:
            text_emotion_config = self.config_manager.config.get("models", {}).get("text_emotion", {})
            text_emotion_config["use_local_models"] = use_local
            self.text_emotion_model = TextEmotionModel(text_emotion_config)
            if self.text_emotion_model.load_model():
                print("âœ… é˜¶æ®µ2: æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»æ¨¡åž‹åŠ è½½æˆåŠŸ")
            else:
                print("âŒ é˜¶æ®µ2: æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»æ¨¡åž‹åŠ è½½å¤±è´¥")
        except Exception as e:
            print(f"âŒ é˜¶æ®µ2: æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»æ¨¡åž‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.text_emotion_model = None
        
        # é˜¶æ®µ3: emotion2vec å£°å­¦æƒ…æ„Ÿåˆ†æž
        try:
            audio_emotion_config = self.config_manager.config.get("models", {}).get("emotion2vec", {})
            audio_emotion_config["use_local_models"] = use_local
            self.audio_emotion_model = AudioEmotionModel(audio_emotion_config)
            if self.audio_emotion_model.load_model():
                print("âœ… é˜¶æ®µ3: emotion2vec å£°å­¦æƒ…æ„Ÿåˆ†æžæ¨¡åž‹åŠ è½½æˆåŠŸ")
            else:
                print("âŒ é˜¶æ®µ3: emotion2vec å£°å­¦æƒ…æ„Ÿåˆ†æžæ¨¡åž‹åˆå§‹åŒ–å¤±è´¥")
        except Exception as e:
            print(f"âŒ é˜¶æ®µ3: emotion2vec å£°å­¦æƒ…æ„Ÿåˆ†æžæ¨¡åž‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.audio_emotion_model = None

    def _load_models(self):
        try:
            if not self.text_emotion_model.load_model():
                logger.warning("æ–‡æœ¬æƒ…æ„Ÿåˆ†æžæ¨¡åž‹åŠ è½½å¤±è´¥")
            if not self.audio_emotion_model.load_model():
                logger.warning("éŸ³é¢‘æƒ…æ„Ÿåˆ†æžæ¨¡åž‹åŠ è½½å¤±è´¥")
            if not self.asr_model.load_model():
                logger.warning("ASRæ¨¡åž‹åŠ è½½å¤±è´¥")
            logger.info("æ¨¡åž‹åŠ è½½å®Œæˆ")
        except Exception as e:
            logger.error(f"æ¨¡åž‹åŠ è½½å¤±è´¥: {str(e)}")
    
    async def process_text_service(self, text: str, language: str = "zh", style_preference: Optional[str] = None) -> Dict[str, Any]:
        start_time = time.time()
        try:
            logger.info(f"å¼€å§‹å¤„ç†æ–‡å­—æƒ…æ„Ÿåˆ†æžï¼Œè¾“å…¥é•¿åº¦: {len(text)}")
            emotion_result = self.text_emotion_model.analyze(text)
            if not emotion_result:
                raise EmotionAnalysisError("æ–‡æœ¬æƒ…æ„Ÿåˆ†æžå¤±è´¥ï¼ŒæœªèŽ·å¾—æœ‰æ•ˆç»“æžœ")
            emotion_tags = self._extract_text_emotion_tags(emotion_result)
            confidence = self._extract_confidence(emotion_result)
            generated_text = await self._generate_text_with_llm(text, emotion_tags, style_preference, language)
            image_prompt = self._build_image_prompt(text, emotion_tags, generated_text)
            image_result = self.image_generator.generate(prompt=image_prompt, save_local=True)
            image_path = image_result['local_paths'][0] if image_result and image_result.get('local_paths') else None
            processing_time = time.time() - start_time
            result = {
                'text': text,
                'emotion_tags': emotion_tags,
                'emotion_confidence': confidence,
                'generated_content': {'text': generated_text, 'image_path': image_path, 'style': style_preference or 'default', 'metadata': {'language': language,'style_preference': style_preference,'generation_timestamp': datetime.utcnow().isoformat()}},
                'processing_time': round(processing_time, 3),
                'status': 'success'
            }
            return result
        except Exception as e:
            logger.error(f"æ–‡å­—å¤„ç†æœåŠ¡å¤±è´¥: {str(e)}", exc_info=True)
            raise EmotionAnalysisError(f"æ–‡å­—å¤„ç†å¤±è´¥: {str(e)}")
    
    async def process_audio_service(self, audio_data: bytes, language: str = "zh", enable_dual_analysis: bool = True, fusion_strategy: str = "weighted") -> Dict[str, Any]:
        start_time = time.time()
        temp_audio_path = None
        try:
            temp_audio_path = self._save_temp_audio(audio_data)
            transcribed_text = self.asr_model.transcribe(temp_audio_path)
            if not transcribed_text:
                raise AudioProcessingError("è¯­éŸ³è¯†åˆ«å¤±è´¥")
            audio_emotion_result = self.audio_emotion_model.analyze(temp_audio_path)
            audio_emotion_tags = self._extract_audio_emotion_tags(audio_emotion_result)
            text_emotion_result = self.text_emotion_model.analyze(transcribed_text)
            text_emotion_tags = self._extract_text_emotion_tags(text_emotion_result)
            merged_emotion = self._fuse_emotions(audio_emotion_tags, text_emotion_tags, fusion_strategy)
            generated_text = await self._generate_text_with_llm(transcribed_text, merged_emotion, None, language)
            image_prompt = self._build_image_prompt(transcribed_text, merged_emotion, generated_text)
            image_result = self.image_generator.generate(prompt=image_prompt, save_local=True)
            image_path = image_result['local_paths'][0] if image_result and image_result.get('local_paths') else None
            processing_time = time.time() - start_time
            result = {
                'transcribed_text': transcribed_text,
                'emotion_analysis': {'audio_emotion': audio_emotion_tags,'text_emotion': text_emotion_tags,'merged_emotion': merged_emotion,'fusion_rules': fusion_strategy},
                'generated_content': {'text': generated_text,'image_path': image_path,'style': 'default','metadata': {'language': language,'dual_analysis': enable_dual_analysis,'fusion_strategy': fusion_strategy,'generation_timestamp': datetime.utcnow().isoformat()}},
                'processing_time': round(processing_time, 3),
                'status': 'success'
            }
            return result
        except Exception as e:
            logger.error(f"éŸ³é¢‘å¤„ç†æœåŠ¡å¤±è´¥: {str(e)}", exc_info=True)
            raise AudioProcessingError(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {str(e)}")
        finally:
            if temp_audio_path and os.path.exists(temp_audio_path):
                self._cleanup_temp_file(temp_audio_path)

    def _extract_audio_emotion_tags(self, emotion_result: List[Dict[str, Any]]) -> List[str]:
        tags = []
        try:
            for item in emotion_result:
                if isinstance(item, dict) and 'raw_result' in item:
                    raw = item['raw_result']
                    if isinstance(raw, dict):
                        if 'emotion' in raw:
                            emotion_id = raw['emotion']
                            if isinstance(emotion_id, int) and emotion_id in self.EMOTION2VEC_LABELS:
                                tags.append(self.EMOTION2VEC_LABELS[emotion_id])
                        elif 'emotions' in raw:
                            emotions = raw['emotions']
                            if isinstance(emotions, list):
                                for emotion in emotions[:3]:
                                    if isinstance(emotion, dict) and 'emotion' in emotion:
                                        emotion_id = emotion['emotion']
                                        if isinstance(emotion_id, int) and emotion_id in self.EMOTION2VEC_LABELS:
                                            tags.append(self.EMOTION2VEC_LABELS[emotion_id])
        except Exception as e:
            logger.warning(f"æå–éŸ³é¢‘æƒ…æ„Ÿæ ‡ç­¾å¤±è´¥: {str(e)}")
        return tags[:3] if tags else ['neutral']

    def _extract_text_emotion_tags(self, emotion_result: List[Dict[str, Any]]) -> List[str]:
        tags = []
        try:
            for item in emotion_result:
                if isinstance(item, dict):
                    if 'label' in item:
                        label = item['label']
                        if label in self.TEXT_EMOTION_LABELS.values():
                            tags.append(label)
                    elif 'emotion' in item:
                        emotion = item['emotion']
                        if emotion in self.TEXT_EMOTION_LABELS.values():
                            tags.append(emotion)
        except Exception as e:
            logger.warning(f"æå–æ–‡æœ¬æƒ…æ„Ÿæ ‡ç­¾å¤±è´¥: {str(e)}")
        return tags[:3] if tags else ['neutral']

    def _extract_confidence(self, emotion_result: List[Dict[str, Any]]) -> float:
        try:
            for item in emotion_result:
                if isinstance(item, dict) and 'score' in item:
                    return float(item['score'])
        except Exception:
            pass
        return 0.8

    def _build_image_prompt(self, text: str, emotion_tags: List[str], generated_text: str) -> str:
        emotion_desc = ", ".join(emotion_tags)
        return f"åŸºäºŽæ–‡å­—'{text}'å’Œæƒ…æ„Ÿ'{emotion_desc}'ï¼Œç”Ÿæˆä¸Žæ–‡æ¡ˆ'{generated_text}'ç›¸åŒ¹é…çš„å›¾ç‰‡"

    def _fuse_emotions(self, audio_emotions: List[str], text_emotions: List[str], strategy: str) -> List[str]:
        if strategy == "max":
            return audio_emotions if len(audio_emotions) > len(text_emotions) else text_emotions
        elif strategy == "average":
            intersection = set(audio_emotions) & set(text_emotions)
            if intersection:
                return list(intersection)
            else:
                return list(set(audio_emotions + text_emotions))
        else:  # weighted
            all_emotions = audio_emotions + text_emotions
            emotion_count = {}
            for emotion in all_emotions:
                if emotion in audio_emotions and emotion in text_emotions:
                    emotion_count[emotion] = emotion_count.get(emotion, 0) + 2
                else:
                    emotion_count[emotion] = emotion_count.get(emotion, 0) + 1
            sorted_emotions = sorted(emotion_count.items(), key=lambda x: x[1], reverse=True)
            return [emotion for emotion, _ in sorted_emotions[:3]]
    async def _generate_text_with_llm(self, text: str, emotion_tags: List[str], style: Optional[str], language: str) -> str:
        emotion_desc = ", ".join(emotion_tags)
        style_desc = f"é£Žæ ¼ï¼š{style}" if style else ""
        return f"åŸºäºŽæ‚¨çš„æƒ…æ„Ÿ'{emotion_desc}'ï¼Œæˆ‘ä¸º'{text}'åˆ›ä½œäº†è¿™æ®µæ–‡æ¡ˆï¼šåœ¨{emotion_desc}çš„æ—‹å¾‹ä¸­ï¼Œ{text}ä»¿ä½›æœ‰äº†æ–°çš„ç”Ÿå‘½..."

    def _save_temp_audio(self, audio_data: bytes) -> str:
        import tempfile
        temp_dir = "data/temp"
        os.makedirs(temp_dir, exist_ok=True)
        temp_file = tempfile.NamedTemporaryFile(dir=temp_dir, suffix=".wav", delete=False)
        temp_file.write(audio_data)
        temp_file.close()
        return temp_file.name

    def _cleanup_temp_file(self, file_path: str):
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
                logger.info(f"ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†: {file_path}")
        except Exception as e:
            logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")

    async def run_three_stage_analysis(self, audio_path: str) -> Dict[str, Any]:
        """è¿è¡Œä¸‰é˜¶æ®µæƒ…æ„Ÿåˆ†æž"""
        start_time = time.time()
        try:
            # é˜¶æ®µ1: ASRè½¬å½•
            if not self.asr_model:
                raise AudioProcessingError("ASRæ¨¡åž‹æœªåˆå§‹åŒ–")
            
            transcribed_text = self.asr_model.transcribe(audio_path)
            if not transcribed_text:
                raise AudioProcessingError("è¯­éŸ³è¯†åˆ«å¤±è´¥")
            
            # é˜¶æ®µ2: æ–‡æœ¬æƒ…æ„Ÿåˆ†æž
            if not self.text_emotion_model:
                raise EmotionAnalysisError("æ–‡æœ¬æƒ…æ„Ÿåˆ†æžæ¨¡åž‹æœªåˆå§‹åŒ–")
            
            text_emotion_result = self.text_emotion_model.analyze(transcribed_text)
            text_emotion_tags = self._extract_text_emotion_tags(text_emotion_result)
            
            # é˜¶æ®µ3: éŸ³é¢‘æƒ…æ„Ÿåˆ†æž
            if not self.audio_emotion_model:
                raise EmotionAnalysisError("éŸ³é¢‘æƒ…æ„Ÿåˆ†æžæ¨¡åž‹æœªåˆå§‹åŒ–")
            
            audio_emotion_result = self.audio_emotion_model.analyze(audio_path)
            audio_emotion_tags = self._extract_audio_emotion_tags(audio_emotion_result)
            
            # èžåˆæƒ…æ„Ÿæ ‡ç­¾
            merged_emotion = self._fuse_emotions(audio_emotion_tags, text_emotion_tags, "weighted")
            
            # ç”Ÿæˆæ–‡æ¡ˆå’Œå›¾ç‰‡
            generated_content = await self._generate_content(transcribed_text, merged_emotion)
            
            processing_time = time.time() - start_time
            
            return {
                'transcribed_text': transcribed_text,
                'emotion_analysis': {
                    'audio_emotion': audio_emotion_tags,
                    'text_emotion': text_emotion_tags,
                    'merged_emotion': merged_emotion,
                    'fusion_strategy': 'weighted'
                },
                'generated_content': generated_content,
                'processing_time': round(processing_time, 3),
                'status': 'success'
            }
            
        except Exception as e:
            logger.error(f"ä¸‰é˜¶æ®µæƒ…æ„Ÿåˆ†æžå¤±è´¥: {str(e)}", exc_info=True)
            raise EmotionAnalysisError(f"æƒ…æ„Ÿåˆ†æžå¤±è´¥: {str(e)}")

    async def run_text_emotion_analysis(self, text: str) -> Dict[str, Any]:
        """è¿è¡Œçº¯æ–‡æœ¬æƒ…æ„Ÿåˆ†æž"""
        start_time = time.time()
        try:
            # æ–‡æœ¬æƒ…æ„Ÿåˆ†æž
            if not self.text_emotion_model:
                raise EmotionAnalysisError("æ–‡æœ¬æƒ…æ„Ÿåˆ†æžæ¨¡åž‹æœªåˆå§‹åŒ–")
            
            text_emotion_result = self.text_emotion_model.analyze(text)
            text_emotion_tags = self._extract_text_emotion_tags(text_emotion_result)
            
            # ç”Ÿæˆæ–‡æ¡ˆå’Œå›¾ç‰‡
            generated_content = await self._generate_content(text, text_emotion_tags)
            
            processing_time = time.time() - start_time
            
            return {
                'input_text': text,
                'emotion_analysis': {
                    'text_emotion': text_emotion_tags,
                    'confidence': self._extract_confidence(text_emotion_result)
                },
                'generated_content': generated_content,
                'processing_time': round(processing_time, 3),
                'status': 'success'
            }
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬æƒ…æ„Ÿåˆ†æžå¤±è´¥: {str(e)}", exc_info=True)
            raise EmotionAnalysisError(f"æ–‡æœ¬æƒ…æ„Ÿåˆ†æžå¤±è´¥: {str(e)}")

    async def _generate_content(self, text: str, emotion_tags: List[str]) -> Dict[str, Any]:
        """ç”Ÿæˆæ–‡æ¡ˆå’Œå›¾ç‰‡å†…å®¹"""
        try:
            # ç”Ÿæˆæ–‡æ¡ˆ
            generated_text = ""
            if self.text_generator:
                try:
                    generated_text = await self.text_generator.generate_text(text, emotion_tags)
                except Exception as e:
                    logger.warning(f"æ–‡æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
                    generated_text = f"åŸºäºŽ'{text}'çš„æƒ…æ„Ÿ'{', '.join(emotion_tags)}'ï¼Œæˆ‘ä¸ºæ‚¨åˆ›ä½œäº†è¿™æ®µæ–‡æ¡ˆï¼šåœ¨{', '.join(emotion_tags)}çš„æ—‹å¾‹ä¸­ï¼Œ{text}ä»¿ä½›æœ‰äº†æ–°çš„ç”Ÿå‘½..."
            else:
                generated_text = f"åŸºäºŽ'{text}'çš„æƒ…æ„Ÿ'{', '.join(emotion_tags)}'ï¼Œæˆ‘ä¸ºæ‚¨åˆ›ä½œäº†è¿™æ®µæ–‡æ¡ˆï¼šåœ¨{', '.join(emotion_tags)}çš„æ—‹å¾‹ä¸­ï¼Œ{text}ä»¿ä½›æœ‰äº†æ–°çš„ç”Ÿå‘½..."
            
            # ç”Ÿæˆå›¾ç‰‡
            image_path = None
            if self.image_generator:
                try:
                    image_prompt = self._build_image_prompt(text, emotion_tags, generated_text)
                    image_result = self.image_generator.generate(prompt=image_prompt, save_local=True)
                    image_path = image_result['local_paths'][0] if image_result and image_result.get('local_paths') else None
                except Exception as e:
                    logger.warning(f"å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {e}")
            
            return {
                'text': generated_text,
                'image_path': image_path,
                'style': 'default',
                'metadata': {
                    'generation_timestamp': datetime.now(timezone.utc).isoformat(),
                    'emotion_tags': emotion_tags
                }
            }
            
        except Exception as e:
            logger.error(f"å†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}")
            return {
                'text': f"åŸºäºŽ'{text}'çš„æƒ…æ„Ÿåˆ†æžç»“æžœç”Ÿæˆæ–‡æ¡ˆå¤±è´¥",
                'image_path': None,
                'style': 'default',
                'metadata': {
                    'generation_timestamp': datetime.now(timezone.utc).isoformat(),
                    'emotion_tags': emotion_tags,
                    'error': str(e)
                }
            }




class DoubaoVLMClient:
    """
    è½»é‡ HTTP å®¢æˆ·ç«¯ï¼šè°ƒç”¨è±†åŒ…å¤šæ¨¡æ€ chat/completions æŽ¥å£åšå›¾åƒç†è§£
    - è¯»å– ConfigManager é‡Œçš„ vlm é…ç½®ï¼ˆbase_url/api_key_env/model_nameï¼‰
    - è¿”å›žç»“æž„åŒ– JSONï¼ˆcaption/objects/styles/suggestions/edit_prompt/negative_promptï¼‰
    """

    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.provider: str = "doubao"
        self.model: str = "doubao-1.5-vision-lite-250315"
        self.base_url: str = ""
        self.api_key: Optional[str] = None
        self.completions_url: str = ""
        self._setup_client()

    def _setup_client(self) -> None:
        """å¯¹é½å¼åˆå§‹åŒ–ï¼šä»Ž config_manager è¯»å–é…ç½®å¹¶å®Œæˆå¿…éœ€å­—æ®µå‡†å¤‡"""
        try:
            models_cfg = (self.config_manager.config or {}).get("image_models", {}) or {}
            vlm_cfg = models_cfg.get("vlm", {}) or {}


            # use_api å…³æŽ‰å°±ç›´æŽ¥æ‹’ç»
            if vlm_cfg.get("use_api", True) is not True:
                raise RuntimeError("DoubaoVLMClient: å½“å‰é…ç½®æœªå¯ç”¨ VLM APIï¼ˆmodels.vlm.use_api != trueï¼‰")

            # 2) åŸºæœ¬å­—æ®µ
            self.provider = vlm_cfg.get("provider", "doubao")
            self.model = vlm_cfg.get("model_name", "doubao-1.5-vision-lite-250315")

            api_conf = vlm_cfg.get("api", {}) or {}
            base_url_raw = api_conf.get("base_url", "")
            if not base_url_raw:
                raise RuntimeError("DoubaoVLMClient: ç¼ºå°‘ models.vlm.api.base_url")

            self.base_url = base_url_raw.rstrip("/")

            # 3) API Keyï¼šä¼˜å…ˆçŽ¯å¢ƒå˜é‡ï¼Œå…¶æ¬¡æ”¯æŒåœ¨é…ç½®ä¸­ç›´å¡«
            key_env = api_conf.get("api_key_env", "ARK_API_KEY")
            self.api_key = os.getenv(key_env) or api_conf.get("api_key")
            if not self.api_key:
                raise RuntimeError(
                    f"DoubaoVLMClient: æœªæ‰¾åˆ° API Keyï¼Œè¯·è®¾ç½®çŽ¯å¢ƒå˜é‡ {key_env} æˆ–åœ¨ models.vlm.api.api_key ä¸­ç›´å¡«"
                )

            # 4) ç»ˆç«¯ç‚¹ï¼ˆOpenAI å…¼å®¹ï¼‰
            self.completions_url = f"{self.base_url}/chat/completions"
            logger.info("DoubaoVLMClient åˆå§‹åŒ–å®Œæˆ: provider=%s, model=%s, base_url=%s", self.provider, self.model, self.base_url)

        except Exception as e:
            logger.error("DoubaoVLMClient åˆå§‹åŒ–å¤±è´¥: %s", e, exc_info=True)
            raise
    
    @staticmethod
    def _guess_mime(image_bytes: bytes) -> Tuple[str, str]:
        """ç®€å•åˆ¤æ–­ mimeï¼ˆä¾¿äºŽå¤šæ¨¡æ€ä¸Šä¼ ï¼‰"""
        try:
            with Image.open(io.BytesIO(image_bytes)) as im:
                fmt = (im.format or "PNG").upper()
                if fmt == "JPG":
                    fmt = "JPEG"
                mime = f"image/{fmt.lower()}"
                ext = fmt.lower()
                return mime, ext
        except Exception:
            return "image/png", "png"


    def analyze_image(
        self,
        image_bytes: bytes,
        intent: str = "enhance",           # ä¾‹å¦‚ enhance / replace_bg / recolor ...
        style_preset: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        è®©æ¨¡åž‹â€œçœ‹å›¾+ç»™ç¼–è¾‘å»ºè®®+ç”Ÿæˆç¼–è¾‘ promptâ€ã€‚è¦æ±‚æ¨¡åž‹è¾“å‡º JSONã€‚
        è¿”å›žå­—æ®µï¼š
          caption / objects / styles / colors / suggestions / edit_prompt / negative_prompt
        """
        # å‡†å¤‡å›¾ç‰‡ï¼ˆbase64ï¼‰
        mime, ext = DoubaoVLMClient._guess_mime(image_bytes)
        b64 = base64.b64encode(image_bytes).decode("utf-8")

        system_prompt = (
            "ä½ æ˜¯èµ„æ·±è§†è§‰è®¾è®¡å¸ˆå’Œä¿®å›¾æŒ‡å¯¼ï¼Œæ“…é•¿ä»Žå›¾ç‰‡ä¸­æå–å…³é”®ä¿¡æ¯å¹¶ç”Ÿæˆé€‚åˆå›¾åƒç¼–è¾‘æ¨¡åž‹çš„æŒ‡ä»¤ã€‚\n"
            "ä¸¥æ ¼ä»¥ JSON è¿”å›žï¼Œé”®åŒ…æ‹¬ï¼šcaption, objects, styles, colors, suggestions, edit_prompt, negative_promptã€‚"
        )
        user_instruction = (
            f"è¯·åˆ†æžè¿™å¼ å›¾ç‰‡ï¼Œä»»åŠ¡æ„å›¾ï¼š{intent}ï¼›"
            f"é£Žæ ¼é¢„è®¾ï¼ˆå¯é€‰ï¼‰ï¼š{style_preset or 'æ— '}ã€‚"
            "è¦æ±‚ï¼š\n"
            "1) captionï¼šä¸€å¥è¯æè¿°å›¾ç‰‡ä¸»ä½“ä¸Žåœºæ™¯ï¼›\n"
            "2) objectsï¼šæœ€å¤š5ä¸ªå…³é”®å¯¹è±¡åŠå…¶å±žæ€§ï¼ˆåç§°/é¢œè‰²/æè´¨/å¯è§ç‰¹å¾ï¼‰ï¼›\n"
            "3) stylesï¼šæå–3~6ä¸ªé£Žæ ¼æ ‡ç­¾ï¼ˆå¦‚ cinematicã€neonã€portrait ç­‰ï¼‰ï¼›\n"
            "4) colorsï¼š2~5ä¸ªä¸»è‰²ï¼›\n"
            "5) suggestionsï¼š3æ¡å¯æ‰§è¡Œçš„ç¼–è¾‘å»ºè®®ï¼ˆé¢å‘ i2i æ¨¡åž‹ï¼‰ï¼›\n"
            "6) edit_promptï¼šç»¼åˆä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆå¯ç›´æŽ¥ç”¨äºŽå›¾åƒç¼–è¾‘æ¨¡åž‹çš„è‹±æ–‡æŒ‡ä»¤ï¼ˆæè¿°æ¸…æ™°ã€é¿å…ä¸»è§‚è¯ã€åŒ…å«å…‰ç…§/è´¨æ„Ÿ/å±‚æ¬¡ç­‰ï¼‰ï¼›\n"
            "7) negative_promptï¼šå¸¸è§ä¼ªå½±/ä¸æœŸæœ›æ•ˆæžœã€‚\n"
            "åªè¾“å‡º JSONï¼Œä¸è¦é¢å¤–è§£é‡Šã€‚"
        )

        data_url = f"data:{mime};base64,{b64}"

        # æŒ‰ OpenAI å¤šæ¨¡æ€é£Žæ ¼ç»„ç»‡æ¶ˆæ¯ï¼ˆè±†åŒ… ark v3 åŸºæœ¬å…¼å®¹ï¼›å¦‚æœ‰å·®å¼‚ä»…éœ€å¾®è°ƒå­—æ®µåï¼‰
        messages = [
        # system å»ºè®®ç›´æŽ¥ç”¨çº¯å­—ç¬¦ä¸²ï¼Œæ›´å…¼å®¹
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": [
            {"type": "image_url", "image_url": {"url": data_url}},
            {"type": "text", "text": user_instruction}
        ]}
        ]

        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": 0.2,
            "response_format": {"type": "json_object"}  # è¦æ±‚èµ° JSON
        }
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        try:
            resp = requests.post(self.completions_url, headers=headers, data=json.dumps(payload), timeout=60)
            resp.raise_for_status()
            data = resp.json()
            # é€‚é…å¸¸è§è¿”å›žï¼šchoices[0].message.content
            content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
            result = json.loads(content) if content else {}
            # åŸºæœ¬å…œåº•
            result.setdefault("caption", "")
            result.setdefault("objects", [])
            result.setdefault("styles", [])
            result.setdefault("colors", [])
            result.setdefault("suggestions", [])
            result.setdefault("edit_prompt", result.get("caption", ""))
            result.setdefault("negative_prompt", "low-res, blurry, artifacts, oversaturated, watermark")
            return result
        except Exception as e:
            logger.error(f"DoubaoVLMClient.analyze_image å¤±è´¥: {e}", exc_info=True)
            raise ImageProcessingError(f"è±†åŒ…å¤šæ¨¡æ€åˆ†æžå¤±è´¥: {e}")




class ImageEmotionAnalyzerService:
    """
    æ”¹é€ ç‰ˆï¼šä½¿ç”¨è±†åŒ… VLM åšå›¾åƒç†è§£ + äº§å‡ºç¼–è¾‘ promptï¼›
    ç„¶åŽå¯é€‰ç›´æŽ¥è°ƒç”¨ Doubao i2iï¼ˆé€šè¿‡ä½ çš„ ImageEditor é€‚é…å™¨ï¼‰åšä¿®å›¾ã€‚
    """

    def __init__(self, config_manager: ConfigManager):
        self.config_manager = config_manager
        self.vlm = None           # DoubaoVLMClient
        self.image_editor = None  # ä½ å·²æœ‰çš„ i2i æ¨¡åž‹é€‚é…å™¨
        self.defaults: Dict[str, Any] = {}
        self._setup()

    def _setup(self):
        conf = self.config_manager.config.get("models", {}).get("vlm", {})
        if not conf.get("use_api", True):
            raise RuntimeError("å½“å‰é…ç½®æœªå¯ç”¨ VLM API")
        
        self.vlm = DoubaoVLMClient(self.config_manager)

        # è¯»å– i2i é»˜è®¤å€¼ï¼ˆèµ°ä½ å·²æœ‰çš„ doubao i2i æ¨¡åž‹ï¼‰
        img_defaults = conf.get("defaults", {})
        self.defaults = {
            "guidance_scale": float(img_defaults.get("guidance_scale", 7.5)),
            "num_images": int(img_defaults.get("num_images", 1)),
            "size": img_defaults.get("size", "1024x1024"),
            "watermark": bool(img_defaults.get("watermark", True)),
            "save_local": True,
        }
        # ä½ çš„ ImageEditor å·²ç»é›†æˆè±†åŒ… i2iï¼šç›´æŽ¥åˆå§‹åŒ–å³å¯
        self.image_editor = ImageEditor(self.config_manager)
        logger.info("ImageEmotionAnalyzer(Doubao) åˆå§‹åŒ–å®Œæˆ")

    # ========== å¯¹å¤– API ==========

    def analyze_image_bytes(
        self,
        image_bytes: bytes,
        intent: str = "enhance",
        style_preset: Optional[str] = None,
        auto_edit: bool = False
    ) -> Dict[str, Any]:
        """bytes å…¥å£ï¼šè¿”å›žåˆ†æžç»“æžœï¼›å¯é€‰ç›´æŽ¥è‡ªåŠ¨ä¿®å›¾ã€‚"""
        if not validate_image_file(image_bytes):
            raise FileValidationError("æ— æ•ˆçš„å›¾ç‰‡æ–‡ä»¶")
        # åˆ†æž
        analysis = self.vlm.analyze_image(image_bytes, intent=intent, style_preset=style_preset)

        result: Dict[str, Any] = {
            "analysis": analysis,  # caption/objects/styles/colors/suggestions/...
            "status": "ok"
        }

        if auto_edit:
            # å°†åŽŸå›¾å…ˆè½ç›˜åˆ° tempï¼Œä¾¿äºŽ i2i è°ƒç”¨
            temp_dir = self.config_manager.config.get("paths", {}).get("temp_dir", "data/temp")
            os.makedirs(temp_dir, exist_ok=True)
            tmp_path = os.path.join(temp_dir, "i2i_input.png")
            save_upload_file(image_bytes, tmp_path)

            edit_prompt = analysis.get("edit_prompt") or analysis.get("caption") or ""
            neg = analysis.get("negative_prompt", "")
            # ç»„è£… i2i å‚æ•°ï¼ˆä½  ImageEditor çš„ edit_image(...) å¯èƒ½æœ‰ä¸åŒç­¾åï¼Œè¿™é‡ŒæŒ‰ä½ ä¹‹å‰ç¤ºä¾‹æ”¹ï¼‰
            params = {
                "input_path_or_url": tmp_path,
                "prompt": edit_prompt,
                "negative_prompt": neg,
                "guidance_scale": self.defaults["guidance_scale"],
                "save_local": True
            }
            try:
                out = self.image_editor.edit_image(**params)
                result["auto_edit"] = {
                    "prompt": edit_prompt,
                    "negative_prompt": neg,
                    "output": out
                }
            except Exception as e:
                logger.warning(f"è‡ªåŠ¨ä¿®å›¾å¤±è´¥: {e}")
                result["auto_edit"] = {"error": str(e)}
        return result

    def analyze_image_path(
        self,
        image_path: str,
        intent: str = "enhance",
        style_preset: Optional[str] = None,
        auto_edit: bool = False
    ) -> Dict[str, Any]:
        """æ–‡ä»¶è·¯å¾„å…¥å£ï¼šè¯»å–ä¸º bytes åŽå¤ç”¨é€»è¾‘"""
        if not os.path.exists(image_path):
            raise FileNotFoundError(image_path)
        with open(image_path, "rb") as f:
            return self.analyze_image_bytes(
                f.read(),
                intent=intent,
                style_preset=style_preset,
                auto_edit=auto_edit
            )



        """
        å›¾ç‰‡å†…å®¹ï¼š{image_content},
        å½“å‰æƒ…æ„Ÿï¼š{}ï¼Œ
        none
        ä¿®æ”¹æ„è§ï¼šä½ æ˜¯ä¸€ä¸ªå°çº¢ä¹¦xxxxx
        """
# class ImageEmotionAnalyzer:

#     def __init__(self, config_manager: ConfigManager):
#         self.config_manager = config_manager
#         self.vlm_model: Optional[Interrogator] = None
#         self.vlm_cfg: Optional[CIConfig] = None
#         self.setup_models()

#     # ---------------------------
#     # åˆå§‹åŒ–ä¸Žæ¨¡åž‹è£…è½½
#     # ---------------------------
#     def setup_models(self):
#         """åˆå§‹åŒ– clip_interrogator"""
#         print("ðŸš€ æ­£åœ¨åˆå§‹åŒ– Image â†’ Prompt åˆ†æžå™¨ï¼ˆclip_interrogatorï¼‰...")
#         try:
#             ci_conf = self._build_ci_config()
#             self.vlm_cfg = ci_conf
#             self.vlm_model = Interrogator(ci_conf)
#             # è½»é‡ VRAM ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰
#             if self._get_bool("apply_low_vram_defaults", default=True):
#                 self.vlm_model.config.apply_low_vram_defaults()

#             print(f"âœ… clip_interrogator åŠ è½½æˆåŠŸï¼ŒCLIP: {self.vlm_model.config.clip_model_name}")
#         except Exception as e:
#             print(f"âŒ clip_interrogator åˆå§‹åŒ–å¤±è´¥: {e}")
#             logger.exception("clip_interrogator åˆå§‹åŒ–å¤±è´¥")
#             self.vlm_model = None

#     def _build_ci_config(self) -> CIConfig:
#         """
#         ä»Žé¡¹ç›®é…ç½®æž„å»º clip_interrogator çš„ Config
#         æœŸæœ›çš„é…ç½®ç¤ºä¾‹ï¼š
#         {
#           "models": {
#             "clip_interrogator": {
#               "clip_model_name": "ViT-L-14/openai",
#               "blip_model": "blip-base",     # å¯ä¸å¡«
#               "cache_path": "./data/ci_cache",
#               "device": "cuda:0",            # æˆ– "cpu"
#               "chunk_size": 2048
#             }
#           },
#           "settings": {
#             "use_local_models": True,
#             "apply_low_vram_defaults": True
#           }
#         }
#         """
#         ci_cfg = self.config_manager.config.get("models", {}).get("clip_interrogator", {})
#         clip_model_name = ci_cfg.get("clip_model_name", "ViT-L-14/openai")
#         cache_path = ci_cfg.get("cache_path")  # None = é»˜è®¤
#         device = ci_cfg.get("device")          # None = è‡ªåŠ¨
#         chunk_size = ci_cfg.get("chunk_size")  # None = é»˜è®¤

#         conf = CIConfig(clip_model_name=clip_model_name)
#         if cache_path:
#             conf.cache_path = cache_path
#         if device:
#             conf.device = device
#         if chunk_size:
#             conf.chunk_size = int(chunk_size)
#         # å…¶ä½™å¯æ ¹æ®éœ€è¦ç»§ç»­æ˜ å°„ï¼ˆä¾‹å¦‚: blip_model ç­‰ï¼‰
#         return conf

#     def _get_bool(self, key: str, default: bool) -> bool:
#         return bool(self.config_manager.config.get("settings", {}).get(key, default))

#     # # ---------------------------
#     # # å¯¹å¤–ä¸»æŽ¥å£
#     # # ---------------------------
#     # def analyze_image_bytes(
#     #     self,
#     #     image_bytes: bytes,
#     #     intent: str = "enhance",
#     #     style_preset: Optional[str] = None,
#     #     need_negative: bool = True,
#     # ) -> Dict[str, Any]:
#     #     """
#     #     è¾“å…¥ï¼šåŽŸå§‹å›¾åƒ bytes
#     #     è¾“å‡ºï¼šå¯ç›´æŽ¥ç”¨äºŽ T2I / I2I çš„ prompt åŠç»“æž„åŒ–ä¿¡æ¯
#     #     """
#     #     if not self.vlm_model:
#     #         raise ImageProcessingError("clip_interrogator æœªåˆå§‹åŒ–")

#     #     if not validate_image_file(image_bytes):
#     #         raise FileValidationError("æ— æ•ˆçš„å›¾ç‰‡æ–‡ä»¶æ ¼å¼")

#     #     try:
#     #         img = Image.open(io.BytesIO(image_bytes)).convert("RGB")
#     #         return self._analyze_pil_image(
#     #             img,
#     #             intent=intent,
#     #             style_preset=style_preset,
#     #             need_negative=need_negative,
#     #         )
#     #     except Exception as e:
#     #         logger.exception("analyze_image_bytes å¤±è´¥")
#     #         raise ImageProcessingError(f"å›¾åƒåˆ†æžå¤±è´¥: {e}")

#     def analyze_image_path(
#         self,
#         image_path: str,
#         intent: str = "enhance",
#         style_preset: Optional[str] = None,
#         need_negative: bool = True,
#     ) -> Dict[str, Any]:
#         """è¾“å…¥ï¼šå›¾ç‰‡è·¯å¾„"""
#         if not self.vlm_model:
#             raise ImageProcessingError("clip_interrogator æœªåˆå§‹åŒ–")
#         try:
#             img = Image.open(image_path).convert("RGB")
#             return self._analyze_pil_image(
#                 img,
#                 intent=intent,
#                 style_preset=style_preset,
#                 need_negative=need_negative,
#             )
#         except Exception as e:
#             logger.exception("analyze_image_path å¤±è´¥")
#             raise ImageProcessingError(f"å›¾åƒåˆ†æžå¤±è´¥: {e}")

#     # ---------------------------
#     # å†…éƒ¨ï¼šæ ¸å¿ƒåˆ†æžä¸Ž Prompt ç»„è£…
#     # ---------------------------
#     def _analyze_pil_image(
#         self,
#         img: Image.Image,
#         intent: str,
#         style_preset: Optional[str],
#         need_negative: bool,
#     ) -> Dict[str, Any]:
#         """
#         ç”¨ CI ç”ŸæˆåŸºç¡€æè¿°ï¼Œå†æ‹¼æˆé€‚åˆç¼–è¾‘/ç”Ÿæˆçš„ promptï¼ˆå¯ç›´æŽ¥æŽ¥ä½ çš„ ImageServiceï¼‰
#         """
#         # 1) åŸºç¡€æè¿°ï¼ˆCI çš„ä¸€å¥è¯ promptï¼‰
#         ci_prompt = (self.vlm_model.generate_caption(img)
#              if hasattr(self.vlm_model, "generate_caption")
#              else self.vlm_model.caption(img))
        
#         # 2) é¢å¤–ä¿¡å·ï¼ˆå¯é€‰ï¼štags / å¼ºé£Žæ ¼è¯ï¼›è¿™é‡Œç”¨ç®€å•å¯å‘å¼ï¼‰
#         tags = self._extract_style_tags(ci_prompt)

#         # 3) ç»„è£…æœ€ç»ˆå¯ç”¨ prompt
#         prompt, negative = self._compose_prompt(
#             base_caption=ci_prompt,
#             intent=intent,
#             style_preset=style_preset,
#             tags=tags,
#             need_negative=need_negative,
#         )

#         result: Dict[str, Any] = {
#             "prompt": prompt,
#             "negative_prompt": negative if need_negative else "",
#             "meta": {
#                 "caption": ci_prompt,
#                 "tags": tags,
#                 "intent": intent,
#                 "style_preset": style_preset,
#                 "model_info": self.get_model_info(),
#             }
#         }
#         return result

#     @staticmethod
#     def _extract_style_tags(ci_prompt: str) -> List[str]:
#         """
#         éžä¸¥æ ¼ï¼šåŸºäºŽå…³é”®å­—çš„è½»é‡é£Žæ ¼è¯æŠ½å–
#         ä½ ä¹Ÿå¯ä»¥ç»´æŠ¤ä¸€ä¸ªè¯å…¸/æ­£åˆ™æˆ–æŠŠè¿™ä¸€æ­¥äº¤ç»™æ›´å¼ºçš„æ–‡æœ¬æ¨¡åž‹åšæ¸…æ´—
#         """
#         vocab = {
#             "cinematic": ["cinematic", "film", "movie", "anamorphic"],
#             "vintage": ["vintage", "retro", "film grain"],
#             "neon": ["neon", "cyberpunk", "glow"],
#             "portrait": ["portrait", "headshot", "bokeh"],
#             "macro": ["macro", "close-up"],
#             "studio": ["studio", "softbox", "rim light", "key light"],
#             "fantasy": ["fantasy", "mythical", "epic"],
#             "watercolor": ["watercolor", "ink", "gouache"],
#             "3d": ["3d", "render", "octane", "unreal"],
#         }
#         tags: List[str] = []
#         low = ci_prompt.lower()
#         for tag, kws in vocab.items():
#             if any(k in low for k in kws):
#                 tags.append(tag)
#         return tags[:6]

#     @staticmethod
#     def _compose_prompt(
#         base_caption: str,
#         intent: str,
#         style_preset: Optional[str],
#         tags: List[str],
#         need_negative: bool,
#     ) -> Tuple[str, str]:
#         """
#         æœ€ç»ˆ prompt ç»„åˆç­–ç•¥ï¼ˆå°½é‡â€œå³æ’å³ç”¨â€ï¼‰
#         - base_captionï¼šCI äº§å‡ºçš„æè¿°
#         - intentï¼šå¯¹é½ä½ çš„ä¸šåŠ¡ç›®æ ‡ï¼Œå¦‚ enhance / replace_bg / recolor / upstyle / anime ç­‰
#         - style_presetï¼šå¤–éƒ¨ä¼ å…¥é£Žæ ¼é¢„è®¾ï¼ˆè¦†ç›–æˆ–è¿½åŠ ï¼‰
#         """
#         parts: List[str] = [base_caption.strip()]

#         # æ ¹æ®æ„å›¾è¡¥ä¸€äº›å¸¸ç”¨çš„â€œç¼–è¾‘å‹å¥½ä¿®é¥°è¯â€
#         intent_snippets = {
#             "enhance": "high detail, natural lighting, realistic shading, finely textured",
#             "replace_bg": "clean subject separation, seamless background blending, consistent lighting",
#             "recolor": "color-consistent, natural hue transitions, texture preserved",
#             "anime": "anime style, clean lines, vibrant colors, high contrast",
#             "sketch": "pencil sketch, cross-hatching, monochrome shading",
#         }
#         if intent in intent_snippets:
#             parts.append(intent_snippets[intent])

#         # æ ‡ç­¾è¯
#         if tags:
#             parts.append(", ".join(tags))

#         # é£Žæ ¼é¢„è®¾
#         if style_preset:
#             parts.append(style_preset)

#         prompt = ", ".join([p for p in parts if p])

#         # Negative promptï¼ˆæŒ‰éœ€ï¼‰
#         negative = ""
#         if need_negative:
#             negative = (
#                 "low-res, blurry, artifacts, overexposed, oversaturated, "
#                 "extra limbs, duplicated objects, distorted proportions, watermark, text artifacts"
#             )
#         return prompt, negative

#     # ---------------------------
#     # è¾…åŠ©ï¼šæ¨¡åž‹ä¿¡æ¯
#     # ---------------------------
#     def get_model_info(self) -> Dict[str, Any]:
#         clip_name = self.vlm_cfg.clip_model_name if self.vlm_cfg else "unknown"
#         device = getattr(self.vlm_cfg, "device", "auto") if self.vlm_cfg else "auto"
#         return {
#             "backend": "clip_interrogator",
#             "clip_model": clip_name,
#             "device": device,
#         }

    